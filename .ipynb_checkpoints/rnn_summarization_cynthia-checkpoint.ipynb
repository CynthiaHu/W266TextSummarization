{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from cytoolz import concatv\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk,pprint\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "# import rnnlm; reload(rnnlm)\n",
    "# import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read nyt flat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Baghdad Gallery Owner Hopes Culture Can Dispel Hate'\", \"'Sweet and Sour Sit Down to Dessert'\"]\n"
     ]
    }
   ],
   "source": [
    "# need to remove quote?\n",
    "# !tail -n 1000 data/nyt_structured_data.txt > data/nyt_test.txt\n",
    "file = open('data/nyt_test.txt','rt')\n",
    "read_array = file.readlines()\n",
    "title=[]\n",
    "lead=[]\n",
    "body=[]\n",
    "for line in read_array:\n",
    "    data = line.split(' , ') #file id, headline, leading_paragraph and full_text\n",
    "    title.append(data[1])\n",
    "    lead.append(data[2])\n",
    "    body.append(data[3])\n",
    "file.close()\n",
    "print(title[:2])\n",
    "# print(body[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'RHUBARB is an alarmingly sour vegetable passed off as a fruit, but requiring a huge mound of sugar to effect the transformation.Crumb cake is a huge mound of sugar disguised as a cake, but demanding a bracing counterpoint -- say a swallow of coffee or tea -- to allay its cloying sweetness.' 292\n"
     ]
    }
   ],
   "source": [
    "print(lead[1], len(lead[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'He', 'may', 'be', 'the', 'last', 'hopeful', 'man', 'in', 'Iraq', '.', 'Amid', 'the', 'violence', ',', 'the', 'crumbling', 'economy', 'and', 'rising']\n"
     ]
    }
   ],
   "source": [
    "# tokenize built in load_data function\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "t = title[1]\n",
    "b = body[1]\n",
    "# print(word_tokenize(b))\n",
    "tokens = []\n",
    "body_tokens = []\n",
    "title_tokens = []\n",
    "for i in range(len(body)):\n",
    "    tokens.extend(nltk.wordpunct_tokenize(body[i]))\n",
    "    body_tokens.append(nltk.wordpunct_tokenize(body[i]))\n",
    "    title_tokens.append(nltk.wordpunct_tokenize(title[i]))\n",
    "# tokens = tokens[20:] #select tokens\n",
    "# text = nltk.Text(tokens)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'\",\n",
       "  'Baghdad',\n",
       "  'Gallery',\n",
       "  'Owner',\n",
       "  'Hopes',\n",
       "  'Culture',\n",
       "  'Can',\n",
       "  'Dispel',\n",
       "  'Hate',\n",
       "  \"'\"],\n",
       " [\"'\", 'Sweet', 'and', 'Sour', 'Sit', 'Down', 'to', 'Dessert', \"'\"]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'RHUBARB',\n",
       " 'is',\n",
       " 'an',\n",
       " 'alarmingly',\n",
       " 'sour',\n",
       " 'vegetable',\n",
       " 'passed',\n",
       " 'off',\n",
       " 'as',\n",
       " 'a',\n",
       " 'fruit',\n",
       " ',',\n",
       " 'but',\n",
       " 'requiring',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'mound',\n",
       " 'of',\n",
       " 'sugar',\n",
       " 'to',\n",
       " 'effect',\n",
       " 'the',\n",
       " 'transformation',\n",
       " '.',\n",
       " 'Crumb',\n",
       " 'cake',\n",
       " 'is',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'mound',\n",
       " 'of',\n",
       " 'sugar',\n",
       " 'disguised',\n",
       " 'as',\n",
       " 'a',\n",
       " 'cake',\n",
       " ',',\n",
       " 'but',\n",
       " 'demanding',\n",
       " 'a',\n",
       " 'bracing',\n",
       " 'counterpoint',\n",
       " '--',\n",
       " 'say',\n",
       " 'a',\n",
       " 'swallow',\n",
       " 'of',\n",
       " 'coffee',\n",
       " 'or',\n",
       " 'tea',\n",
       " '--',\n",
       " 'to',\n",
       " 'allay',\n",
       " 'its',\n",
       " 'cloying',\n",
       " 'sweetness',\n",
       " '.',\n",
       " 'These',\n",
       " 'two',\n",
       " 'truths',\n",
       " 'coexisted',\n",
       " 'in',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'without',\n",
       " 'overlapping',\n",
       " 'until',\n",
       " 'I',\n",
       " 'bit',\n",
       " 'into',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'so',\n",
       " 'texturally',\n",
       " 'perfect',\n",
       " '(',\n",
       " 'soft',\n",
       " 'sliver',\n",
       " 'of',\n",
       " 'cake',\n",
       " 'topped',\n",
       " 'by',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'layer',\n",
       " 'of',\n",
       " 'grape',\n",
       " '-',\n",
       " 'size',\n",
       " 'crumbs',\n",
       " '),',\n",
       " 'yet',\n",
       " 'so',\n",
       " 'toothachingly',\n",
       " 'sweet',\n",
       " 'that',\n",
       " 'the',\n",
       " 'only',\n",
       " 'antidote',\n",
       " 'was',\n",
       " 'sucking',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lemon',\n",
       " 'in',\n",
       " 'my',\n",
       " 'seltzer',\n",
       " '.',\n",
       " 'The',\n",
       " 'sourness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lemon',\n",
       " 'immediately',\n",
       " 'made',\n",
       " 'me',\n",
       " 'think',\n",
       " 'about',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'I',\n",
       " 'had',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fridge',\n",
       " '.',\n",
       " 'It',\n",
       " 'occurred',\n",
       " 'to',\n",
       " 'me',\n",
       " 'that',\n",
       " ',',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'cutting',\n",
       " 'its',\n",
       " 'tartness',\n",
       " 'with',\n",
       " 'a',\n",
       " 'mountain',\n",
       " 'of',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'why',\n",
       " 'not',\n",
       " 'mix',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'into',\n",
       " 'a',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'to',\n",
       " 'cut',\n",
       " 'the',\n",
       " 'cake',\n",
       " \"'\",\n",
       " 's',\n",
       " 'sweetness',\n",
       " '?',\n",
       " 'It',\n",
       " 'was',\n",
       " 'an',\n",
       " \"''\",\n",
       " 'Aha',\n",
       " \"!''\",\n",
       " 'moment',\n",
       " ',',\n",
       " 'as',\n",
       " 'when',\n",
       " 'some',\n",
       " 'forebear',\n",
       " 'first',\n",
       " 'paired',\n",
       " 'caviar',\n",
       " 'with',\n",
       " 'Champagne',\n",
       " '.',\n",
       " 'Or',\n",
       " 'when',\n",
       " 'peanut',\n",
       " 'butter',\n",
       " 'met',\n",
       " 'jelly',\n",
       " '.',\n",
       " 'A',\n",
       " 'little',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'background',\n",
       " 'is',\n",
       " 'in',\n",
       " 'order',\n",
       " '.',\n",
       " 'I',\n",
       " \"'\",\n",
       " 've',\n",
       " 'spent',\n",
       " 'a',\n",
       " 'good',\n",
       " 'part',\n",
       " 'of',\n",
       " 'my',\n",
       " 'adult',\n",
       " 'life',\n",
       " 'actively',\n",
       " 'pursuing',\n",
       " 'the',\n",
       " 'perfect',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'recipe',\n",
       " ':',\n",
       " 'a',\n",
       " 'high',\n",
       " 'ratio',\n",
       " 'of',\n",
       " 'meltingly',\n",
       " 'tender',\n",
       " 'crumbs',\n",
       " 'to',\n",
       " 'buttery',\n",
       " ',',\n",
       " 'velvety',\n",
       " 'cake',\n",
       " '.',\n",
       " 'But',\n",
       " 'that',\n",
       " 'requires',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'was',\n",
       " 'determined',\n",
       " 'to',\n",
       " 'find',\n",
       " 'a',\n",
       " 'formula',\n",
       " 'that',\n",
       " 'had',\n",
       " 'the',\n",
       " 'crumb',\n",
       " 'without',\n",
       " 'the',\n",
       " 'cloy',\n",
       " '.',\n",
       " 'Did',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'epiphany',\n",
       " 'mean',\n",
       " 'that',\n",
       " 'perfection',\n",
       " 'was',\n",
       " 'within',\n",
       " 'my',\n",
       " 'grasp',\n",
       " '?',\n",
       " 'I',\n",
       " 'added',\n",
       " 'cubed',\n",
       " 'rhubarb',\n",
       " 'to',\n",
       " 'the',\n",
       " 'cake',\n",
       " 'batter',\n",
       " ',',\n",
       " 'and',\n",
       " 'covered',\n",
       " 'it',\n",
       " 'all',\n",
       " 'with',\n",
       " 'giant',\n",
       " 'crumbs',\n",
       " 'using',\n",
       " 'a',\n",
       " 'technique',\n",
       " 'lifted',\n",
       " 'from',\n",
       " 'Cook',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Illustrated',\n",
       " 'magazine',\n",
       " '.',\n",
       " 'It',\n",
       " 'calls',\n",
       " 'for',\n",
       " 'making',\n",
       " 'a',\n",
       " 'homogenous',\n",
       " 'brown',\n",
       " 'sugar',\n",
       " 'dough',\n",
       " ',',\n",
       " 'then',\n",
       " 'pinching',\n",
       " 'off',\n",
       " 'marbles',\n",
       " 'to',\n",
       " 'form',\n",
       " 'crumbs',\n",
       " '.',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'slightly',\n",
       " 'more',\n",
       " 'time',\n",
       " '-',\n",
       " 'consuming',\n",
       " 'than',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'streusel',\n",
       " '-',\n",
       " 'making',\n",
       " 'method',\n",
       " 'of',\n",
       " 'pulsing',\n",
       " 'the',\n",
       " 'ingredients',\n",
       " 'in',\n",
       " 'a',\n",
       " 'food',\n",
       " 'processor',\n",
       " 'and',\n",
       " 'dumping',\n",
       " 'them',\n",
       " 'on',\n",
       " 'the',\n",
       " 'batter',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'it',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'a',\n",
       " 'magnificent',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " ',',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'happy',\n",
       " 'to',\n",
       " 'put',\n",
       " 'in',\n",
       " '10',\n",
       " 'extra',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'But',\n",
       " 'the',\n",
       " 'cake',\n",
       " 'was',\n",
       " 'still',\n",
       " 'cloying',\n",
       " '.',\n",
       " 'The',\n",
       " 'rhubarb',\n",
       " 'had',\n",
       " 'become',\n",
       " 'acidic',\n",
       " 'puddles',\n",
       " 'of',\n",
       " 'pulp',\n",
       " ':',\n",
       " 'no',\n",
       " 'peanut',\n",
       " 'butter',\n",
       " '-',\n",
       " 'and',\n",
       " '-',\n",
       " 'jelly',\n",
       " 'harmony',\n",
       " '.',\n",
       " 'So',\n",
       " 'I',\n",
       " 'contemplated',\n",
       " 'all',\n",
       " 'the',\n",
       " 'successful',\n",
       " 'rhubarb',\n",
       " 'desserts',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'd',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'until',\n",
       " 'I',\n",
       " 'hit',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'rhubarb',\n",
       " 'crisp',\n",
       " 'made',\n",
       " 'by',\n",
       " 'Claudia',\n",
       " 'Fleming',\n",
       " 'when',\n",
       " 'she',\n",
       " 'was',\n",
       " 'at',\n",
       " 'Gramercy',\n",
       " 'Tavern',\n",
       " '.',\n",
       " '(',\n",
       " 'Disclosure',\n",
       " ':',\n",
       " 'I',\n",
       " 'wrote',\n",
       " 'a',\n",
       " 'cookbook',\n",
       " 'with',\n",
       " 'her',\n",
       " ').',\n",
       " 'I',\n",
       " 'remembered',\n",
       " 'her',\n",
       " 'saying',\n",
       " 'that',\n",
       " 'tossing',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'in',\n",
       " 'just',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sugar',\n",
       " 'encourages',\n",
       " 'the',\n",
       " 'sturdy',\n",
       " 'stems',\n",
       " 'to',\n",
       " 'absorb',\n",
       " 'the',\n",
       " 'syrup',\n",
       " 'that',\n",
       " 'forms',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'exactly',\n",
       " 'what',\n",
       " 'I',\n",
       " 'needed',\n",
       " 'my',\n",
       " 'rhubarb',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " 'Though',\n",
       " 'I',\n",
       " 'was',\n",
       " 'loath',\n",
       " 'to',\n",
       " 'add',\n",
       " 'more',\n",
       " 'sugar',\n",
       " 'to',\n",
       " 'a',\n",
       " 'recipe',\n",
       " 'that',\n",
       " 'was',\n",
       " 'already',\n",
       " 'overloaded',\n",
       " ',',\n",
       " 'I',\n",
       " 'did',\n",
       " 'it',\n",
       " 'anyway',\n",
       " '.',\n",
       " 'When',\n",
       " 'rhubarb',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'No',\n",
       " '.',\n",
       " '2',\n",
       " 'cooled',\n",
       " ',',\n",
       " 'I',\n",
       " 'dug',\n",
       " 'in',\n",
       " '.',\n",
       " 'The',\n",
       " 'rhubarb',\n",
       " 'was',\n",
       " 'mellow',\n",
       " 'and',\n",
       " 'gently',\n",
       " 'sweet',\n",
       " ',',\n",
       " 'with',\n",
       " 'still',\n",
       " 'enough',\n",
       " 'zesty',\n",
       " 'bite',\n",
       " 'to',\n",
       " 'offset',\n",
       " 'the',\n",
       " 'sugary',\n",
       " 'cake',\n",
       " '.',\n",
       " 'At',\n",
       " 'last',\n",
       " 'I',\n",
       " 'had',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'fulfillment',\n",
       " '.',\n",
       " 'Which',\n",
       " 'just',\n",
       " 'goes',\n",
       " 'to',\n",
       " 'show',\n",
       " ':',\n",
       " 'when',\n",
       " 'less',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'more',\n",
       " ',',\n",
       " 'try',\n",
       " 'adding',\n",
       " 'more',\n",
       " '.',\n",
       " 'Rhubarb',\n",
       " \"'\",\n",
       " 'Big',\n",
       " 'Crumb',\n",
       " \"'\",\n",
       " 'Coffeecake',\n",
       " 'Time',\n",
       " ':',\n",
       " '11',\n",
       " '/',\n",
       " '2',\n",
       " 'hours',\n",
       " ',',\n",
       " 'plus',\n",
       " 'coolingButter',\n",
       " 'for',\n",
       " 'greasing',\n",
       " 'pan',\n",
       " 'For',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'filling',\n",
       " ':',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'pound',\n",
       " 'rhubarb',\n",
       " ',',\n",
       " 'trimmed',\n",
       " '1',\n",
       " '/',\n",
       " '4',\n",
       " 'cup',\n",
       " 'sugar',\n",
       " '2',\n",
       " 'teaspoons',\n",
       " 'cornstarch',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'ground',\n",
       " 'gingerFor',\n",
       " 'the',\n",
       " 'crumbs',\n",
       " ':',\n",
       " '1',\n",
       " '/',\n",
       " '3',\n",
       " 'cup',\n",
       " 'dark',\n",
       " 'brown',\n",
       " 'sugar',\n",
       " '1',\n",
       " '/',\n",
       " '3',\n",
       " 'cup',\n",
       " 'granulated',\n",
       " 'sugar',\n",
       " '1',\n",
       " 'teaspoon',\n",
       " 'ground',\n",
       " 'cinnamon',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'ground',\n",
       " 'ginger',\n",
       " '1',\n",
       " '/',\n",
       " '8',\n",
       " 'teaspoon',\n",
       " 'salt',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'cup',\n",
       " 'melted',\n",
       " 'butter',\n",
       " '13',\n",
       " '/',\n",
       " '4',\n",
       " 'cups',\n",
       " 'cake',\n",
       " 'flourFor',\n",
       " 'the',\n",
       " 'cake',\n",
       " ':',\n",
       " '1',\n",
       " '/',\n",
       " '3',\n",
       " 'cup',\n",
       " 'sour',\n",
       " 'cream',\n",
       " '1',\n",
       " 'large',\n",
       " 'egg',\n",
       " '1',\n",
       " 'large',\n",
       " 'egg',\n",
       " 'yolk',\n",
       " '2',\n",
       " 'teaspoons',\n",
       " 'vanilla',\n",
       " 'extract',\n",
       " '1',\n",
       " 'cup',\n",
       " 'cake',\n",
       " 'flour',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'cup',\n",
       " 'sugar',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'baking',\n",
       " 'soda',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'baking',\n",
       " 'powder',\n",
       " '1',\n",
       " '/',\n",
       " '4',\n",
       " 'teaspoon',\n",
       " 'salt',\n",
       " '6',\n",
       " 'tablespoons',\n",
       " 'softened',\n",
       " 'butter',\n",
       " ',',\n",
       " 'cut',\n",
       " 'into',\n",
       " '8',\n",
       " 'pieces',\n",
       " '.',\n",
       " '1',\n",
       " '.',\n",
       " 'Preheat',\n",
       " 'oven',\n",
       " 'to',\n",
       " '325',\n",
       " 'degrees',\n",
       " '.',\n",
       " 'Grease',\n",
       " 'an',\n",
       " '8',\n",
       " '-',\n",
       " 'inch',\n",
       " '-',\n",
       " 'square',\n",
       " 'baking',\n",
       " 'pan',\n",
       " '.',\n",
       " 'For',\n",
       " 'filling',\n",
       " ',',\n",
       " 'slice',\n",
       " 'rhubarb',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'inch',\n",
       " 'thick',\n",
       " 'and',\n",
       " 'toss',\n",
       " 'with',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'cornstarch',\n",
       " 'and',\n",
       " 'ginger',\n",
       " '.',\n",
       " 'Set',\n",
       " 'aside',\n",
       " '.',\n",
       " '2',\n",
       " '.',\n",
       " 'To',\n",
       " 'make',\n",
       " 'crumbs',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'large',\n",
       " 'bowl',\n",
       " ',',\n",
       " 'whisk',\n",
       " 'together',\n",
       " 'sugars',\n",
       " ',',\n",
       " 'spices',\n",
       " ',',\n",
       " 'salt',\n",
       " 'and',\n",
       " 'butter',\n",
       " 'until',\n",
       " 'smooth',\n",
       " '.',\n",
       " 'Stir',\n",
       " 'in',\n",
       " 'flour',\n",
       " 'with',\n",
       " 'a',\n",
       " 'spatula',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'look',\n",
       " 'like',\n",
       " 'a',\n",
       " 'solid',\n",
       " 'dough',\n",
       " '.',\n",
       " '3',\n",
       " '.',\n",
       " 'To',\n",
       " 'prepare',\n",
       " 'cake',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'small',\n",
       " 'bowl',\n",
       " ',',\n",
       " 'stir',\n",
       " 'together',\n",
       " 'the',\n",
       " 'sour',\n",
       " 'cream',\n",
       " ',',\n",
       " 'egg',\n",
       " ',',\n",
       " 'egg',\n",
       " 'yolk',\n",
       " 'and',\n",
       " 'vanilla',\n",
       " '.',\n",
       " 'Using',\n",
       " 'a',\n",
       " 'mixer',\n",
       " 'fitted',\n",
       " 'with',\n",
       " 'paddle',\n",
       " 'attachment',\n",
       " ',',\n",
       " 'mix',\n",
       " 'together',\n",
       " 'flour',\n",
       " ',',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'baking',\n",
       " 'soda',\n",
       " ',',\n",
       " 'baking',\n",
       " 'powder',\n",
       " 'and',\n",
       " 'salt',\n",
       " '.',\n",
       " 'Add',\n",
       " 'butter',\n",
       " 'and',\n",
       " 'a',\n",
       " 'spoonful',\n",
       " 'of',\n",
       " 'sour',\n",
       " 'cream',\n",
       " 'mixture',\n",
       " 'and',\n",
       " 'mix',\n",
       " 'on',\n",
       " 'medium',\n",
       " 'speed',\n",
       " 'until',\n",
       " 'flour',\n",
       " 'is',\n",
       " 'moistened',\n",
       " '.',\n",
       " 'Increase',\n",
       " 'speed',\n",
       " 'and',\n",
       " 'beat',\n",
       " 'for',\n",
       " '30',\n",
       " 'seconds',\n",
       " '.',\n",
       " 'Add',\n",
       " 'remaining',\n",
       " 'sour',\n",
       " 'cream',\n",
       " 'mixture',\n",
       " 'in',\n",
       " 'two',\n",
       " 'batches',\n",
       " ',',\n",
       " 'beating',\n",
       " 'for',\n",
       " '20',\n",
       " 'seconds',\n",
       " 'after',\n",
       " 'each',\n",
       " 'addition',\n",
       " ',',\n",
       " 'and',\n",
       " 'scraping',\n",
       " 'down',\n",
       " 'the',\n",
       " 'sides',\n",
       " 'of',\n",
       " 'bowl',\n",
       " 'with',\n",
       " 'a',\n",
       " 'spatula',\n",
       " '.',\n",
       " 'Scoop',\n",
       " 'out',\n",
       " 'about',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'cup',\n",
       " 'batter',\n",
       " 'and',\n",
       " 'set',\n",
       " 'aside',\n",
       " '.',\n",
       " '4',\n",
       " '.',\n",
       " 'Scrape',\n",
       " 'remaining',\n",
       " 'batter',\n",
       " 'into',\n",
       " 'prepared',\n",
       " 'pan',\n",
       " '.',\n",
       " 'Spoon',\n",
       " 'rhubarb',\n",
       " 'over',\n",
       " 'batter',\n",
       " '.',\n",
       " 'Dollop',\n",
       " 'set',\n",
       " '-',\n",
       " 'aside',\n",
       " 'batter',\n",
       " 'over',\n",
       " 'rhubarb',\n",
       " ';',\n",
       " 'it',\n",
       " 'does',\n",
       " 'not',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'even',\n",
       " '.',\n",
       " '5',\n",
       " '.',\n",
       " 'Using',\n",
       " 'your',\n",
       " 'fingers',\n",
       " ',',\n",
       " 'break',\n",
       " 'topping',\n",
       " 'mixture',\n",
       " 'into',\n",
       " 'big',\n",
       " 'crumbs',\n",
       " ',',\n",
       " 'about',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'inch',\n",
       " 'to',\n",
       " '3',\n",
       " '/',\n",
       " '4',\n",
       " 'inch',\n",
       " 'in',\n",
       " 'size',\n",
       " '.',\n",
       " 'They',\n",
       " 'do',\n",
       " 'not',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'uniform',\n",
       " ',',\n",
       " 'but',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'most',\n",
       " 'are',\n",
       " 'around',\n",
       " 'that',\n",
       " 'size',\n",
       " '.',\n",
       " 'Sprinkle',\n",
       " 'over',\n",
       " 'cake',\n",
       " '.',\n",
       " 'Bake',\n",
       " 'cake',\n",
       " 'until',\n",
       " 'a',\n",
       " 'toothpick',\n",
       " 'inserted',\n",
       " 'into',\n",
       " 'center',\n",
       " 'comes',\n",
       " 'out',\n",
       " 'clean',\n",
       " 'of',\n",
       " 'batter',\n",
       " '(',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'moist',\n",
       " 'from',\n",
       " 'rhubarb',\n",
       " '),',\n",
       " '45',\n",
       " 'to',\n",
       " '55',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'Cool',\n",
       " 'completely',\n",
       " 'before',\n",
       " 'serving',\n",
       " '.',\n",
       " 'Yield',\n",
       " ':',\n",
       " '6',\n",
       " 'to',\n",
       " '8',\n",
       " 'servings',\n",
       " '.',\n",
       " 'A',\n",
       " 'GOOD',\n",
       " 'APPETITE',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stem\n",
    "# # do we need to do stemming?\n",
    "# porter = nltk.PorterStemmer()\n",
    "# lancaster = nltk.LancasterStemmer()\n",
    "# [porter.stem(t) for t in tokens]\n",
    "# # [lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'I',\n",
       " 'THOUGHT',\n",
       " 'I',\n",
       " 'was',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'about',\n",
       " 'energy',\n",
       " 'conservation',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'turns',\n",
       " 'out',\n",
       " 'that',\n",
       " 'I',\n",
       " \"'\",\n",
       " 've',\n",
       " 'been',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'a',\n",
       " 'hypocrite',\n",
       " '.',\n",
       " 'I',\n",
       " 'drive',\n",
       " 'a',\n",
       " 'reasonably',\n",
       " 'fuel',\n",
       " '-',\n",
       " 'efficient',\n",
       " 'car',\n",
       " ',',\n",
       " 'I',\n",
       " 'work',\n",
       " 'at',\n",
       " 'home',\n",
       " 'so',\n",
       " 'I',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'use',\n",
       " 'fuel',\n",
       " 'to',\n",
       " 'commute',\n",
       " 'and',\n",
       " 'I',\n",
       " 'am',\n",
       " 'replacing',\n",
       " 'incandescent',\n",
       " 'bulbs',\n",
       " 'in',\n",
       " 'my',\n",
       " 'home',\n",
       " 'with',\n",
       " 'energy',\n",
       " '-',\n",
       " 'efficient',\n",
       " 'fluorescent',\n",
       " 'bulbs',\n",
       " '.',\n",
       " 'But',\n",
       " 'I',\n",
       " 'am',\n",
       " 'also',\n",
       " 'a',\n",
       " 'prodigious',\n",
       " 'computer',\n",
       " 'user',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'looks',\n",
       " 'as',\n",
       " 'if',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'an',\n",
       " 'energy',\n",
       " 'hog',\n",
       " '.',\n",
       " 'I',\n",
       " 'started',\n",
       " 'checking',\n",
       " 'how',\n",
       " 'much',\n",
       " 'electricity',\n",
       " 'my',\n",
       " 'electronics',\n",
       " 'were',\n",
       " 'consuming',\n",
       " 'when',\n",
       " 'I',\n",
       " 'wasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'using',\n",
       " 'them',\n",
       " '.',\n",
       " 'I',\n",
       " 'used',\n",
       " 'a',\n",
       " 'Kill',\n",
       " 'A',\n",
       " 'Watt',\n",
       " 'EZ',\n",
       " 'energy',\n",
       " 'meter',\n",
       " '(',\n",
       " 'available',\n",
       " 'online',\n",
       " 'for',\n",
       " 'about',\n",
       " '$',\n",
       " '25',\n",
       " ')',\n",
       " 'and',\n",
       " 'began',\n",
       " 'measuring',\n",
       " '.',\n",
       " 'My',\n",
       " 'PC',\n",
       " 'was',\n",
       " 'continuously',\n",
       " 'drawing',\n",
       " '134',\n",
       " 'watts',\n",
       " 'all',\n",
       " 'night',\n",
       " '.',\n",
       " 'The',\n",
       " 'more',\n",
       " 'devices',\n",
       " 'I',\n",
       " 'checked',\n",
       " ',',\n",
       " 'the',\n",
       " 'worse',\n",
       " 'it',\n",
       " 'got',\n",
       " '.',\n",
       " 'My',\n",
       " 'TiVo',\n",
       " 'digital',\n",
       " 'video',\n",
       " 'recorder',\n",
       " 'was',\n",
       " 'sucking',\n",
       " 'down',\n",
       " 'about',\n",
       " '30',\n",
       " 'watts',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'not',\n",
       " 'playing',\n",
       " 'or',\n",
       " 'recording',\n",
       " 'a',\n",
       " 'show',\n",
       " '.',\n",
       " 'A',\n",
       " 'Comcast',\n",
       " 'digital',\n",
       " 'cable',\n",
       " 'set',\n",
       " '-',\n",
       " 'top',\n",
       " 'box',\n",
       " 'made',\n",
       " 'by',\n",
       " 'Motorola',\n",
       " 'that',\n",
       " 'I',\n",
       " 'tested',\n",
       " 'was',\n",
       " 'drawing',\n",
       " 'about',\n",
       " '40',\n",
       " 'watts',\n",
       " '.',\n",
       " 'My',\n",
       " 'DVD',\n",
       " 'player',\n",
       " 'was',\n",
       " 'drawing',\n",
       " '26',\n",
       " 'watts',\n",
       " 'while',\n",
       " 'idle',\n",
       " ',',\n",
       " 'and',\n",
       " 'my',\n",
       " 'audio',\n",
       " 'system',\n",
       " '--',\n",
       " 'which',\n",
       " 'I',\n",
       " 'rarely',\n",
       " 'turned',\n",
       " 'off',\n",
       " '--',\n",
       " 'was',\n",
       " 'using',\n",
       " '47',\n",
       " 'watts',\n",
       " '.',\n",
       " 'This',\n",
       " 'was',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'the',\n",
       " 'numerous',\n",
       " 'power',\n",
       " 'adapters',\n",
       " 'and',\n",
       " 'chargers',\n",
       " ',',\n",
       " 'each',\n",
       " 'drawing',\n",
       " '1',\n",
       " 'or',\n",
       " '2',\n",
       " 'watts',\n",
       " ',',\n",
       " 'not',\n",
       " 'to',\n",
       " 'mention',\n",
       " 'several',\n",
       " 'other',\n",
       " 'devices',\n",
       " 'sipping',\n",
       " 'energy',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'clocks',\n",
       " 'running',\n",
       " 'or',\n",
       " 'to',\n",
       " 'be',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'on',\n",
       " 'at',\n",
       " 'the',\n",
       " 'push',\n",
       " 'of',\n",
       " 'a',\n",
       " 'button',\n",
       " '.',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'partly',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'for',\n",
       " 'the',\n",
       " 'audio',\n",
       " 'system',\n",
       " 'and',\n",
       " 'DVD',\n",
       " 'player',\n",
       " '.',\n",
       " 'They',\n",
       " 'do',\n",
       " 'have',\n",
       " 'on',\n",
       " '/',\n",
       " 'off',\n",
       " 'switches',\n",
       " 'that',\n",
       " 'I',\n",
       " 'was',\n",
       " 'failing',\n",
       " 'to',\n",
       " 'use',\n",
       " '.',\n",
       " 'I',\n",
       " 'had',\n",
       " 'falsely',\n",
       " 'assumed',\n",
       " 'they',\n",
       " 'were',\n",
       " 'using',\n",
       " 'relatively',\n",
       " 'little',\n",
       " 'power',\n",
       " '.',\n",
       " 'But',\n",
       " 'I',\n",
       " 'tested',\n",
       " 'DVR',\n",
       " \"'\",\n",
       " 's',\n",
       " 'from',\n",
       " 'Comcast',\n",
       " ',',\n",
       " 'Dish',\n",
       " 'Network',\n",
       " 'and',\n",
       " 'TiVo',\n",
       " ',',\n",
       " 'and',\n",
       " 'none',\n",
       " 'went',\n",
       " 'into',\n",
       " 'a',\n",
       " 'low',\n",
       " '-',\n",
       " 'power',\n",
       " 'mode',\n",
       " '.',\n",
       " 'All',\n",
       " 'of',\n",
       " 'this',\n",
       " 'wasted',\n",
       " 'power',\n",
       " 'was',\n",
       " 'costing',\n",
       " 'me',\n",
       " 'money',\n",
       " 'and',\n",
       " 'pumping',\n",
       " 'unnecessary',\n",
       " 'CO2',\n",
       " 'into',\n",
       " 'the',\n",
       " 'atmosphere',\n",
       " '.',\n",
       " 'My',\n",
       " 'PC',\n",
       " 'alone',\n",
       " 'was',\n",
       " 'contributing',\n",
       " '2',\n",
       " ',',\n",
       " '000',\n",
       " 'pounds',\n",
       " 'of',\n",
       " 'CO2',\n",
       " 'annually',\n",
       " '.',\n",
       " 'The',\n",
       " 'DVR',\n",
       " '.',\n",
       " 'was',\n",
       " 'adding',\n",
       " 'another',\n",
       " '543',\n",
       " 'pounds',\n",
       " '.',\n",
       " 'Indeed',\n",
       " ',',\n",
       " 'the',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'Energy',\n",
       " 'estimates',\n",
       " 'that',\n",
       " 'in',\n",
       " 'the',\n",
       " 'average',\n",
       " 'home',\n",
       " ',',\n",
       " '40',\n",
       " 'percent',\n",
       " 'of',\n",
       " 'all',\n",
       " 'electricity',\n",
       " 'used',\n",
       " 'to',\n",
       " 'power',\n",
       " 'home',\n",
       " 'electronics',\n",
       " 'is',\n",
       " 'consumed',\n",
       " 'while',\n",
       " 'the',\n",
       " 'products',\n",
       " 'are',\n",
       " 'turned',\n",
       " 'off',\n",
       " '.',\n",
       " 'Add',\n",
       " 'that',\n",
       " 'all',\n",
       " 'up',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'equals',\n",
       " 'the',\n",
       " 'annual',\n",
       " 'output',\n",
       " 'of',\n",
       " '17',\n",
       " 'power',\n",
       " 'plants',\n",
       " ',',\n",
       " 'the',\n",
       " 'government',\n",
       " 'says',\n",
       " '.',\n",
       " 'In',\n",
       " 'an',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'address',\n",
       " 'that',\n",
       " ',',\n",
       " 'a',\n",
       " 'consortium',\n",
       " 'of',\n",
       " 'Intel',\n",
       " ',',\n",
       " 'Google',\n",
       " ',',\n",
       " 'PC',\n",
       " 'makers',\n",
       " 'and',\n",
       " 'other',\n",
       " 'technology',\n",
       " 'companies',\n",
       " 'this',\n",
       " 'week',\n",
       " 'announced',\n",
       " 'their',\n",
       " 'intent',\n",
       " 'to',\n",
       " 'increase',\n",
       " 'the',\n",
       " 'PC',\n",
       " \"'\",\n",
       " 's',\n",
       " 'overall',\n",
       " 'energy',\n",
       " 'efficiency',\n",
       " 'to',\n",
       " '90',\n",
       " 'percent',\n",
       " '.',\n",
       " 'Products',\n",
       " 'that',\n",
       " 'idle',\n",
       " 'in',\n",
       " 'what',\n",
       " 'the',\n",
       " 'industry',\n",
       " 'calls',\n",
       " 'low',\n",
       " '-',\n",
       " 'power',\n",
       " 'mode',\n",
       " ',',\n",
       " 'or',\n",
       " 'lopomo',\n",
       " ',',\n",
       " 'consumed',\n",
       " 'about',\n",
       " '10',\n",
       " 'percent',\n",
       " 'of',\n",
       " 'total',\n",
       " 'electricity',\n",
       " 'in',\n",
       " 'California',\n",
       " 'homes',\n",
       " ',',\n",
       " 'according',\n",
       " 'to',\n",
       " 'a',\n",
       " '2002',\n",
       " 'study',\n",
       " 'prepared',\n",
       " 'for',\n",
       " 'the',\n",
       " 'California',\n",
       " 'Energy',\n",
       " 'Commission',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Lawrence',\n",
       " 'Berkeley',\n",
       " 'National',\n",
       " 'Laboratory',\n",
       " '.',\n",
       " 'A',\n",
       " 'few',\n",
       " 'of',\n",
       " 'those',\n",
       " 'devices',\n",
       " ',',\n",
       " 'even',\n",
       " 'those',\n",
       " 'with',\n",
       " 'Energy',\n",
       " 'Star',\n",
       " 'ratings',\n",
       " 'that',\n",
       " 'signal',\n",
       " 'that',\n",
       " 'they',\n",
       " 'are',\n",
       " 'less',\n",
       " 'wasteful',\n",
       " ',',\n",
       " 'still',\n",
       " 'use',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'power',\n",
       " '.',\n",
       " \"''\",\n",
       " 'Some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'larger',\n",
       " 'big',\n",
       " '-',\n",
       " 'screen',\n",
       " 'TVs',\n",
       " 'consume',\n",
       " 'as',\n",
       " 'much',\n",
       " 'energy',\n",
       " 'each',\n",
       " 'year',\n",
       " 'as',\n",
       " 'a',\n",
       " 'new',\n",
       " 'refrigerator',\n",
       " \",''\",\n",
       " 'according',\n",
       " 'to',\n",
       " 'Noah',\n",
       " 'Horowitz',\n",
       " ',',\n",
       " 'a',\n",
       " 'scientist',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Resources',\n",
       " 'Defense',\n",
       " 'Council',\n",
       " '.',\n",
       " 'You',\n",
       " 'do',\n",
       " 'not',\n",
       " 'have',\n",
       " 'to',\n",
       " 'use',\n",
       " 'an',\n",
       " 'energy',\n",
       " 'meter',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'your',\n",
       " 'consumption',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'turn',\n",
       " 'off',\n",
       " 'your',\n",
       " 'PC',\n",
       " 'when',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'in',\n",
       " 'use',\n",
       " ',',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'it',\n",
       " 'goes',\n",
       " 'into',\n",
       " 'a',\n",
       " 'low',\n",
       " '-',\n",
       " 'power',\n",
       " 'sleep',\n",
       " ',',\n",
       " 'suspend',\n",
       " 'or',\n",
       " 'hibernate',\n",
       " 'mode',\n",
       " '.',\n",
       " 'That',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'always',\n",
       " 'happen',\n",
       " 'automatically',\n",
       " '.',\n",
       " 'Windows',\n",
       " 'XP',\n",
       " 'has',\n",
       " 'both',\n",
       " 'a',\n",
       " 'suspend',\n",
       " 'and',\n",
       " 'hibernate',\n",
       " 'option',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'always',\n",
       " 'turned',\n",
       " 'on',\n",
       " 'by',\n",
       " 'default',\n",
       " '.',\n",
       " 'Computers',\n",
       " 'running',\n",
       " 'the',\n",
       " 'Windows',\n",
       " 'XP',\n",
       " 'operating',\n",
       " 'system',\n",
       " 'can',\n",
       " 'be',\n",
       " 'configured',\n",
       " 'by',\n",
       " 'clicking',\n",
       " 'on',\n",
       " 'Power',\n",
       " 'Options',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Control',\n",
       " 'Panel',\n",
       " 'to',\n",
       " 'set',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'minutes',\n",
       " 'before',\n",
       " 'Windows',\n",
       " 'will',\n",
       " 'turn',\n",
       " 'off',\n",
       " 'the',\n",
       " 'monitor',\n",
       " 'and',\n",
       " 'hard',\n",
       " 'disks',\n",
       " 'or',\n",
       " 'put',\n",
       " 'the',\n",
       " 'system',\n",
       " 'into',\n",
       " 'standby',\n",
       " 'or',\n",
       " 'hibernate',\n",
       " 'mode',\n",
       " '.',\n",
       " '(',\n",
       " 'Hibernation',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'least',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'energy',\n",
       " ').',\n",
       " 'If',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'notebook',\n",
       " 'PC',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'separate',\n",
       " 'settings',\n",
       " 'for',\n",
       " 'when',\n",
       " 'it',\n",
       " 'runs',\n",
       " 'on',\n",
       " 'the',\n",
       " 'battery',\n",
       " 'and',\n",
       " 'when',\n",
       " 'it',\n",
       " 'is',\n",
       " 'plugged',\n",
       " 'in',\n",
       " '.',\n",
       " 'Microsoft',\n",
       " 'says',\n",
       " 'that',\n",
       " 'it',\n",
       " 'has',\n",
       " 'overhauled',\n",
       " 'energy',\n",
       " 'management',\n",
       " 'in',\n",
       " 'its',\n",
       " 'Vista',\n",
       " 'operating',\n",
       " 'system',\n",
       " 'so',\n",
       " 'that',\n",
       " 'machines',\n",
       " ',',\n",
       " 'by',\n",
       " 'default',\n",
       " ',',\n",
       " 'should',\n",
       " 'go',\n",
       " 'into',\n",
       " 'a',\n",
       " 'low',\n",
       " '-',\n",
       " 'power',\n",
       " 'state',\n",
       " 'after',\n",
       " '60',\n",
       " 'minutes',\n",
       " 'of',\n",
       " 'inactivity',\n",
       " '.',\n",
       " 'The',\n",
       " 'PC',\n",
       " 'sips',\n",
       " 'only',\n",
       " 'a',\n",
       " 'few',\n",
       " 'watts',\n",
       " 'until',\n",
       " 'the',\n",
       " 'user',\n",
       " 'touches',\n",
       " 'the',\n",
       " 'mouse',\n",
       " 'or',\n",
       " 'keyboard',\n",
       " '.',\n",
       " 'To',\n",
       " 'configure',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'with',\n",
       " 'Vista',\n",
       " ',',\n",
       " 'type',\n",
       " \"''\",\n",
       " 'Power',\n",
       " 'Options',\n",
       " \"''\",\n",
       " 'in',\n",
       " 'the',\n",
       " 'search',\n",
       " 'box',\n",
       " 'at',\n",
       " 'the',\n",
       " 'bottom',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Start',\n",
       " 'menu',\n",
       " 'and',\n",
       " 'click',\n",
       " 'on',\n",
       " \"''\",\n",
       " 'Change',\n",
       " 'when',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'sleeps',\n",
       " \".''\",\n",
       " 'All',\n",
       " 'of',\n",
       " 'this',\n",
       " ',',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " 'assumes',\n",
       " 'that',\n",
       " 'the',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'working',\n",
       " 'correctly',\n",
       " '.',\n",
       " 'When',\n",
       " 'I',\n",
       " 'first',\n",
       " 'installed',\n",
       " 'Vista',\n",
       " 'on',\n",
       " 'my',\n",
       " 'PC',\n",
       " ',',\n",
       " 'I',\n",
       " 'configured',\n",
       " 'it',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'sleep',\n",
       " 'after',\n",
       " '30',\n",
       " 'minutes',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'has',\n",
       " 'been',\n",
       " 'unreliable',\n",
       " '.',\n",
       " 'Sometimes',\n",
       " 'it',\n",
       " 'fails',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'sleep',\n",
       " ',',\n",
       " 'and',\n",
       " 'at',\n",
       " 'other',\n",
       " 'times',\n",
       " 'it',\n",
       " 'fails',\n",
       " 'to',\n",
       " 'wake',\n",
       " 'up',\n",
       " '.',\n",
       " 'Sometimes',\n",
       " 'I',\n",
       " 'experience',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'of',\n",
       " 'both',\n",
       " 'worlds',\n",
       " ':',\n",
       " 'the',\n",
       " 'drives',\n",
       " 'and',\n",
       " 'fan',\n",
       " 'are',\n",
       " 'spinning',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'monitor',\n",
       " 'is',\n",
       " 'blank',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'cannot',\n",
       " 'get',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'to',\n",
       " 'come',\n",
       " 'back',\n",
       " 'to',\n",
       " 'life',\n",
       " 'without',\n",
       " 'powering',\n",
       " 'it',\n",
       " 'down',\n",
       " 'and',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'back',\n",
       " 'on',\n",
       " '.',\n",
       " 'I',\n",
       " 'spent',\n",
       " 'numerous',\n",
       " 'hours',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'fix',\n",
       " 'the',\n",
       " 'problem',\n",
       " ',',\n",
       " 'including',\n",
       " 'updating',\n",
       " 'the',\n",
       " 'BIOS',\n",
       " ',',\n",
       " 'installing',\n",
       " 'up',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'date',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'all',\n",
       " 'my',\n",
       " 'device',\n",
       " 'drivers',\n",
       " ',',\n",
       " 'checking',\n",
       " 'to',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'there',\n",
       " 'were',\n",
       " 'no',\n",
       " 'unnecessary',\n",
       " 'applications',\n",
       " 'running',\n",
       " 'in',\n",
       " 'the',\n",
       " 'background',\n",
       " 'and',\n",
       " ',',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " 'scanning',\n",
       " 'for',\n",
       " 'spyware',\n",
       " 'and',\n",
       " 'viruses',\n",
       " '.',\n",
       " 'The',\n",
       " 'results',\n",
       " 'were',\n",
       " 'encouraging',\n",
       " '.',\n",
       " 'After',\n",
       " 'all',\n",
       " 'that',\n",
       " 'fiddling',\n",
       " ',',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'went',\n",
       " 'to',\n",
       " 'sleep',\n",
       " 'most',\n",
       " 'nights',\n",
       " 'and',\n",
       " 'woke',\n",
       " 'up',\n",
       " 'most',\n",
       " '--',\n",
       " 'but',\n",
       " 'not',\n",
       " 'all',\n",
       " '--',\n",
       " 'mornings',\n",
       " '.',\n",
       " 'I',\n",
       " 'then',\n",
       " 'installed',\n",
       " 'Co2',\n",
       " 'Saver',\n",
       " '(',\n",
       " 'co2saver',\n",
       " '.',\n",
       " 'snap',\n",
       " '.',\n",
       " 'com',\n",
       " '),',\n",
       " 'a',\n",
       " 'free',\n",
       " 'program',\n",
       " 'for',\n",
       " 'Windows',\n",
       " 'XP',\n",
       " 'and',\n",
       " 'Vista',\n",
       " 'that',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'have',\n",
       " 'solved',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body_tokens)\n",
    "body_tokens[999]\n",
    "# sentence = sent_segment.segment_sentences(body_tokens[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Preprocessing\n",
    "leverage w266_common module\n",
    "tokenize --> canonicalize digit --> canonicalize word --> vocabuluary --> split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/huyue012/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "Vocabulary: 10,000 types\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-fdc2f8728c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# lower case, DG, Unknown, pad sentense start/end, split into training and test data sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# vocab, train_ids, test_ids = utils.load_data(body, title, split=0.8, V=V, shuffle=42)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/W266TextSummarization/w266_common/utils.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(body, title, split, V, shuffle)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_segment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266TextSummarization/sent_segment.py\u001b[0m in \u001b[0;36msegment_sentences\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;34m'punct'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \t'prev-word-is-one-char': prev_one_char}\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mfeaturesets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunct_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'.?!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266TextSummarization/sent_segment.py\u001b[0m in \u001b[0;36mpunct_features\u001b[0;34m(tokens, i)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpunct_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnext_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Helper libraries\n",
    "# assuming body and title are in different arrays, but split training/test not random or shuffled\n",
    "# title not used yet\n",
    "# count sentence or documents?\n",
    "\n",
    "reload(utils)\n",
    "V = 10000\n",
    "# lower case, DG, Unknown, pad sentense start/end, split into training and test data sets\n",
    "# vocab, train_ids, test_ids = utils.load_data(body, title, split=0.8, V=V, shuffle=42)\n",
    "vocab, train_x_ids, test_x_ids, train_y_ids, test_y_ids = utils.load_data(body, title, split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sent_segment # py file\n",
    "all_tokens = []\n",
    "body_tokens = []\n",
    "title_tokens = []\n",
    "for i in range(len(body)):\n",
    "    current_body_tokens= nltk.wordpunct_tokenize(body[i])\n",
    "    all_tokens.extend(current_body_tokens)\n",
    "    body_tokens.append(current_body_tokens)\n",
    "for j in range(len(title)):\n",
    "    current_title_tokens= nltk.wordpunct_tokenize(title[i])\n",
    "    all_tokens.extend(current_title_tokens)\n",
    "    title_tokens.append(current_title_tokens)\n",
    "# vocab = utils.build_vocab(all_tokens, V)\n",
    "\n",
    "# sentence segmentation of the body/lead paragraph\n",
    "\n",
    "\n",
    "sentences = []\n",
    "words = body_tokens[5]\n",
    "print(len(words))\n",
    "i=1217\n",
    "if i+1 >= len(words):\n",
    "    cap = False\n",
    "else:\n",
    "    cap = words[i+1][0].isupper()\n",
    "cap\n",
    "# words[1218][0].isupper()\n",
    "    \n",
    "#     for i, word in enumerate(words):\n",
    "#         print(i, word)\n",
    "# body_tokens[5][i+1][0].isupper()\n",
    "#     sentence = sent_segment.segment_sentences(body_tokens[t])\n",
    "#     sentences.append(sentence)\n",
    "\n",
    "# train_body, test_body, train_title, test_title = get_train_test_doc(sentences, title, split, shuffle)\n",
    "\n",
    "# train_x_ids = preprocess_sentences(train_body, vocab)\n",
    "# test_x_ids = preprocess_sentences(test_body, vocab)\n",
    "# train_y_ids = preprocess_sentences(train_title, vocab)\n",
    "# test_y_ids = preprocess_sentences(test_title, vocab)\n",
    "# sentences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[i+1][0].isupper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embedding\n",
    "For extractive modeling - sentence ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install tensorflow_hub\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "# embeddings = embed([\n",
    "# \"The quick brown fox jumps over the lazy dog.\",\n",
    "# \"I am a sentence for which I would like to get its embedding\"])\n",
    "# session=tf.Session()\n",
    "# session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "# print (tf.Session().run(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Core Graph\n",
    "\n",
    "H : hidden state size = embedding size = per-cell output size\n",
    "\n",
    "May need to update that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Train Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainint The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {self.embedding_encoder_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            self.embedding_encoder_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.encoder_initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, self.encoder_final_h_, train_op] # do i need self.encoder_final_h or decoder??  \n",
    "   \n",
    "        cost = 0.0\n",
    "        vals = session.run(ops, feed_dict)\n",
    "        cost = vals[0] #loss\n",
    "        h = vals[1] #final_h\n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_encoder_time = 25\n",
    "max_decoder_time = 25\n",
    "batch_size = 50\n",
    "learning_rate = 0.01 #default 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a3_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.    \n",
    "        cost = run_epoch(lm, session, bi, \n",
    "                 learning_rate=learning_rate, train=True, \n",
    "                 verbose=True, tick_s=10)\n",
    "        print(\"loss: {:.03f}  (perplexity: {:.02f})\".format(cost, np.exp(cost)))\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "#         if epoch == num_epochs:\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    samples = session.run(lm.pred_samples_, {lm.input_w_: input_w})\n",
    "    final_h = session.run(lm.final_h_, {lm.input_w_: input_w})\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        # Note indexing here: \n",
    "        #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            print(vocab.id_to_word[word_id], end=\" \")\n",
    "            if (i != 0) and (word_id == vocab.START_ID):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score New Data - Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: NMT Tutorial\n",
    "[https://github.com/tensorflow/nmt![image.png](attachment:image.png)]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
