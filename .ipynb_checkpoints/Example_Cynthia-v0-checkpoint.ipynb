{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huyue012/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from cytoolz import concatv\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk,pprint\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "# import rnnlm; reload(rnnlm)\n",
    "# import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['U', '.', 'S', 'TREASURY', \"'\", 'S', 'MULFORD', 'REAFFIRMS', 'G', '-', '6', 'AGREEMENT', 'Treasury', 'Assistant', 'Secretary', 'David', 'Mulford', 'reaffirmed', 'U', '.', 'S', '.', 'backing', 'for', 'the', 'Paris', 'Agreement', 'among', 'six', 'industrial', 'nations', 'to', 'cooperate', 'closely', 'to', 'foster', 'exchange', 'rate', 'stability', 'around', 'current', 'levels', '.'], ['In', 'testimony', 'prepared', 'for', 'delivery', 'before', 'a', 'Senate', 'banking', 'subcommittee', ',', 'Mulford', 'said', 'there', 'was', 'broad', 'recognition', 'in', 'Paris', 'that', '\"', 'further', 'substantial', 'exchange', 'rate', 'shifts', 'could', 'damage', 'growth', 'and', 'adjustment', 'prospects', '.\"'], ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore other data set\n",
    "nltk.corpus.reuters.sents('training/9864')\n",
    "# sentences = np.array(list(reuters.sents('training/9864')), dtype=object)\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read nyt flat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Baghdad Gallery Owner Hopes Culture Can Dispel Hate'\", \"'Sweet and Sour Sit Down to Dessert'\"]\n"
     ]
    }
   ],
   "source": [
    "# need to remove quote?\n",
    "# !tail -n 1000 data/nyt_structured_data.txt > data/nyt_test.txt\n",
    "file = open('data/nyt_test.txt','rt')\n",
    "read_array = file.readlines()\n",
    "title=[]\n",
    "body=[]\n",
    "for line in read_array:\n",
    "    data = line.split(' , ') #file id, headline, leading_paragraph and full_text\n",
    "    title.append(data[1])\n",
    "    body.append(data[3])\n",
    "file.close()\n",
    "print(title[:2])\n",
    "# print(body[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'He', 'may', 'be', 'the', 'last', 'hopeful', 'man', 'in', 'Iraq', '.', 'Amid', 'the', 'violence', ',', 'the', 'crumbling', 'economy', 'and', 'rising']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "t = title[1]\n",
    "b = body[1]\n",
    "# print(word_tokenize(b))\n",
    "tokens = []\n",
    "for i in range(len(body)):\n",
    "    tokens.extend(nltk.wordpunct_tokenize(body[i]))\n",
    "\n",
    "# tokens = tokens[20:] #select tokens\n",
    "# text = nltk.Text(tokens)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'rhubarb',\n",
       " 'is',\n",
       " 'an',\n",
       " 'alarmingli',\n",
       " 'sour',\n",
       " 'veget',\n",
       " 'pass',\n",
       " 'off',\n",
       " 'as',\n",
       " 'a',\n",
       " 'fruit',\n",
       " ',',\n",
       " 'but',\n",
       " 'requir',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'mound',\n",
       " 'of',\n",
       " 'sugar',\n",
       " 'to',\n",
       " 'effect',\n",
       " 'the',\n",
       " 'transform',\n",
       " '.',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'is',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'mound',\n",
       " 'of',\n",
       " 'sugar',\n",
       " 'disguis',\n",
       " 'as',\n",
       " 'a',\n",
       " 'cake',\n",
       " ',',\n",
       " 'but',\n",
       " 'demand',\n",
       " 'a',\n",
       " 'brace',\n",
       " 'counterpoint',\n",
       " '--',\n",
       " 'say',\n",
       " 'a',\n",
       " 'swallow',\n",
       " 'of',\n",
       " 'coffe',\n",
       " 'or',\n",
       " 'tea',\n",
       " '--',\n",
       " 'to',\n",
       " 'allay',\n",
       " 'it',\n",
       " 'cloy',\n",
       " 'sweet',\n",
       " '.',\n",
       " 'these',\n",
       " 'two',\n",
       " 'truth',\n",
       " 'coexist',\n",
       " 'in',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'without',\n",
       " 'overlap',\n",
       " 'until',\n",
       " 'I',\n",
       " 'bit',\n",
       " 'into',\n",
       " 'a',\n",
       " 'piec',\n",
       " 'of',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'so',\n",
       " 'textur',\n",
       " 'perfect',\n",
       " '(',\n",
       " 'soft',\n",
       " 'sliver',\n",
       " 'of',\n",
       " 'cake',\n",
       " 'top',\n",
       " 'by',\n",
       " 'a',\n",
       " 'deep',\n",
       " 'layer',\n",
       " 'of',\n",
       " 'grape',\n",
       " '-',\n",
       " 'size',\n",
       " 'crumb',\n",
       " '),',\n",
       " 'yet',\n",
       " 'so',\n",
       " 'toothachingli',\n",
       " 'sweet',\n",
       " 'that',\n",
       " 'the',\n",
       " 'onli',\n",
       " 'antidot',\n",
       " 'wa',\n",
       " 'suck',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lemon',\n",
       " 'in',\n",
       " 'my',\n",
       " 'seltzer',\n",
       " '.',\n",
       " 'the',\n",
       " 'sour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lemon',\n",
       " 'immedi',\n",
       " 'made',\n",
       " 'me',\n",
       " 'think',\n",
       " 'about',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'I',\n",
       " 'had',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fridg',\n",
       " '.',\n",
       " 'It',\n",
       " 'occur',\n",
       " 'to',\n",
       " 'me',\n",
       " 'that',\n",
       " ',',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'cut',\n",
       " 'it',\n",
       " 'tart',\n",
       " 'with',\n",
       " 'a',\n",
       " 'mountain',\n",
       " 'of',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'whi',\n",
       " 'not',\n",
       " 'mix',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'into',\n",
       " 'a',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'to',\n",
       " 'cut',\n",
       " 'the',\n",
       " 'cake',\n",
       " \"'\",\n",
       " 's',\n",
       " 'sweet',\n",
       " '?',\n",
       " 'It',\n",
       " 'wa',\n",
       " 'an',\n",
       " \"''\",\n",
       " 'aha',\n",
       " \"!''\",\n",
       " 'moment',\n",
       " ',',\n",
       " 'as',\n",
       " 'when',\n",
       " 'some',\n",
       " 'forebear',\n",
       " 'first',\n",
       " 'pair',\n",
       " 'caviar',\n",
       " 'with',\n",
       " 'champagn',\n",
       " '.',\n",
       " 'Or',\n",
       " 'when',\n",
       " 'peanut',\n",
       " 'butter',\n",
       " 'met',\n",
       " 'jelli',\n",
       " '.',\n",
       " 'A',\n",
       " 'littl',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'background',\n",
       " 'is',\n",
       " 'in',\n",
       " 'order',\n",
       " '.',\n",
       " 'I',\n",
       " \"'\",\n",
       " 've',\n",
       " 'spent',\n",
       " 'a',\n",
       " 'good',\n",
       " 'part',\n",
       " 'of',\n",
       " 'my',\n",
       " 'adult',\n",
       " 'life',\n",
       " 'activ',\n",
       " 'pursu',\n",
       " 'the',\n",
       " 'perfect',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'recip',\n",
       " ':',\n",
       " 'a',\n",
       " 'high',\n",
       " 'ratio',\n",
       " 'of',\n",
       " 'meltingli',\n",
       " 'tender',\n",
       " 'crumb',\n",
       " 'to',\n",
       " 'butteri',\n",
       " ',',\n",
       " 'velveti',\n",
       " 'cake',\n",
       " '.',\n",
       " 'but',\n",
       " 'that',\n",
       " 'requir',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'wa',\n",
       " 'determin',\n",
       " 'to',\n",
       " 'find',\n",
       " 'a',\n",
       " 'formula',\n",
       " 'that',\n",
       " 'had',\n",
       " 'the',\n",
       " 'crumb',\n",
       " 'without',\n",
       " 'the',\n",
       " 'cloy',\n",
       " '.',\n",
       " 'did',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'epiphani',\n",
       " 'mean',\n",
       " 'that',\n",
       " 'perfect',\n",
       " 'wa',\n",
       " 'within',\n",
       " 'my',\n",
       " 'grasp',\n",
       " '?',\n",
       " 'I',\n",
       " 'ad',\n",
       " 'cube',\n",
       " 'rhubarb',\n",
       " 'to',\n",
       " 'the',\n",
       " 'cake',\n",
       " 'batter',\n",
       " ',',\n",
       " 'and',\n",
       " 'cover',\n",
       " 'it',\n",
       " 'all',\n",
       " 'with',\n",
       " 'giant',\n",
       " 'crumb',\n",
       " 'use',\n",
       " 'a',\n",
       " 'techniqu',\n",
       " 'lift',\n",
       " 'from',\n",
       " 'cook',\n",
       " \"'\",\n",
       " 's',\n",
       " 'illustr',\n",
       " 'magazin',\n",
       " '.',\n",
       " 'It',\n",
       " 'call',\n",
       " 'for',\n",
       " 'make',\n",
       " 'a',\n",
       " 'homogen',\n",
       " 'brown',\n",
       " 'sugar',\n",
       " 'dough',\n",
       " ',',\n",
       " 'then',\n",
       " 'pinch',\n",
       " 'off',\n",
       " 'marbl',\n",
       " 'to',\n",
       " 'form',\n",
       " 'crumb',\n",
       " '.',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'slightli',\n",
       " 'more',\n",
       " 'time',\n",
       " '-',\n",
       " 'consum',\n",
       " 'than',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'streusel',\n",
       " '-',\n",
       " 'make',\n",
       " 'method',\n",
       " 'of',\n",
       " 'puls',\n",
       " 'the',\n",
       " 'ingredi',\n",
       " 'in',\n",
       " 'a',\n",
       " 'food',\n",
       " 'processor',\n",
       " 'and',\n",
       " 'dump',\n",
       " 'them',\n",
       " 'on',\n",
       " 'the',\n",
       " 'batter',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'it',\n",
       " 'come',\n",
       " 'to',\n",
       " 'a',\n",
       " 'magnific',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " ',',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'happi',\n",
       " 'to',\n",
       " 'put',\n",
       " 'in',\n",
       " '10',\n",
       " 'extra',\n",
       " 'minut',\n",
       " '.',\n",
       " 'but',\n",
       " 'the',\n",
       " 'cake',\n",
       " 'wa',\n",
       " 'still',\n",
       " 'cloy',\n",
       " '.',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'had',\n",
       " 'becom',\n",
       " 'acid',\n",
       " 'puddl',\n",
       " 'of',\n",
       " 'pulp',\n",
       " ':',\n",
       " 'no',\n",
       " 'peanut',\n",
       " 'butter',\n",
       " '-',\n",
       " 'and',\n",
       " '-',\n",
       " 'jelli',\n",
       " 'harmoni',\n",
       " '.',\n",
       " 'So',\n",
       " 'I',\n",
       " 'contempl',\n",
       " 'all',\n",
       " 'the',\n",
       " 'success',\n",
       " 'rhubarb',\n",
       " 'dessert',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'd',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'until',\n",
       " 'I',\n",
       " 'hit',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'rhubarb',\n",
       " 'crisp',\n",
       " 'made',\n",
       " 'by',\n",
       " 'claudia',\n",
       " 'fleme',\n",
       " 'when',\n",
       " 'she',\n",
       " 'wa',\n",
       " 'at',\n",
       " 'gramerci',\n",
       " 'tavern',\n",
       " '.',\n",
       " '(',\n",
       " 'disclosur',\n",
       " ':',\n",
       " 'I',\n",
       " 'wrote',\n",
       " 'a',\n",
       " 'cookbook',\n",
       " 'with',\n",
       " 'her',\n",
       " ').',\n",
       " 'I',\n",
       " 'rememb',\n",
       " 'her',\n",
       " 'say',\n",
       " 'that',\n",
       " 'toss',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'in',\n",
       " 'just',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sugar',\n",
       " 'encourag',\n",
       " 'the',\n",
       " 'sturdi',\n",
       " 'stem',\n",
       " 'to',\n",
       " 'absorb',\n",
       " 'the',\n",
       " 'syrup',\n",
       " 'that',\n",
       " 'form',\n",
       " '.',\n",
       " 'It',\n",
       " 'wa',\n",
       " 'exactli',\n",
       " 'what',\n",
       " 'I',\n",
       " 'need',\n",
       " 'my',\n",
       " 'rhubarb',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " 'though',\n",
       " 'I',\n",
       " 'wa',\n",
       " 'loath',\n",
       " 'to',\n",
       " 'add',\n",
       " 'more',\n",
       " 'sugar',\n",
       " 'to',\n",
       " 'a',\n",
       " 'recip',\n",
       " 'that',\n",
       " 'wa',\n",
       " 'alreadi',\n",
       " 'overload',\n",
       " ',',\n",
       " 'I',\n",
       " 'did',\n",
       " 'it',\n",
       " 'anyway',\n",
       " '.',\n",
       " 'when',\n",
       " 'rhubarb',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'No',\n",
       " '.',\n",
       " '2',\n",
       " 'cool',\n",
       " ',',\n",
       " 'I',\n",
       " 'dug',\n",
       " 'in',\n",
       " '.',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'wa',\n",
       " 'mellow',\n",
       " 'and',\n",
       " 'gentli',\n",
       " 'sweet',\n",
       " ',',\n",
       " 'with',\n",
       " 'still',\n",
       " 'enough',\n",
       " 'zesti',\n",
       " 'bite',\n",
       " 'to',\n",
       " 'offset',\n",
       " 'the',\n",
       " 'sugari',\n",
       " 'cake',\n",
       " '.',\n",
       " 'At',\n",
       " 'last',\n",
       " 'I',\n",
       " 'had',\n",
       " 'crumb',\n",
       " 'cake',\n",
       " 'fulfil',\n",
       " '.',\n",
       " 'which',\n",
       " 'just',\n",
       " 'goe',\n",
       " 'to',\n",
       " 'show',\n",
       " ':',\n",
       " 'when',\n",
       " 'less',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'more',\n",
       " ',',\n",
       " 'tri',\n",
       " 'ad',\n",
       " 'more',\n",
       " '.',\n",
       " 'rhubarb',\n",
       " \"'\",\n",
       " 'big',\n",
       " 'crumb',\n",
       " \"'\",\n",
       " 'coffeecak',\n",
       " 'time',\n",
       " ':',\n",
       " '11',\n",
       " '/',\n",
       " '2',\n",
       " 'hour',\n",
       " ',',\n",
       " 'plu',\n",
       " 'coolingbutt',\n",
       " 'for',\n",
       " 'greas',\n",
       " 'pan',\n",
       " 'for',\n",
       " 'the',\n",
       " 'rhubarb',\n",
       " 'fill',\n",
       " ':',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'pound',\n",
       " 'rhubarb',\n",
       " ',',\n",
       " 'trim',\n",
       " '1',\n",
       " '/',\n",
       " '4',\n",
       " 'cup',\n",
       " 'sugar',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'cornstarch',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'ground',\n",
       " 'gingerfor',\n",
       " 'the',\n",
       " 'crumb',\n",
       " ':',\n",
       " '1',\n",
       " '/',\n",
       " '3',\n",
       " 'cup',\n",
       " 'dark',\n",
       " 'brown',\n",
       " 'sugar',\n",
       " '1',\n",
       " '/',\n",
       " '3',\n",
       " 'cup',\n",
       " 'granul',\n",
       " 'sugar',\n",
       " '1',\n",
       " 'teaspoon',\n",
       " 'ground',\n",
       " 'cinnamon',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'ground',\n",
       " 'ginger',\n",
       " '1',\n",
       " '/',\n",
       " '8',\n",
       " 'teaspoon',\n",
       " 'salt',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'cup',\n",
       " 'melt',\n",
       " 'butter',\n",
       " '13',\n",
       " '/',\n",
       " '4',\n",
       " 'cup',\n",
       " 'cake',\n",
       " 'flourfor',\n",
       " 'the',\n",
       " 'cake',\n",
       " ':',\n",
       " '1',\n",
       " '/',\n",
       " '3',\n",
       " 'cup',\n",
       " 'sour',\n",
       " 'cream',\n",
       " '1',\n",
       " 'larg',\n",
       " 'egg',\n",
       " '1',\n",
       " 'larg',\n",
       " 'egg',\n",
       " 'yolk',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'vanilla',\n",
       " 'extract',\n",
       " '1',\n",
       " 'cup',\n",
       " 'cake',\n",
       " 'flour',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'cup',\n",
       " 'sugar',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'bake',\n",
       " 'soda',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'teaspoon',\n",
       " 'bake',\n",
       " 'powder',\n",
       " '1',\n",
       " '/',\n",
       " '4',\n",
       " 'teaspoon',\n",
       " 'salt',\n",
       " '6',\n",
       " 'tablespoon',\n",
       " 'soften',\n",
       " 'butter',\n",
       " ',',\n",
       " 'cut',\n",
       " 'into',\n",
       " '8',\n",
       " 'piec',\n",
       " '.',\n",
       " '1',\n",
       " '.',\n",
       " 'preheat',\n",
       " 'oven',\n",
       " 'to',\n",
       " '325',\n",
       " 'degre',\n",
       " '.',\n",
       " 'greas',\n",
       " 'an',\n",
       " '8',\n",
       " '-',\n",
       " 'inch',\n",
       " '-',\n",
       " 'squar',\n",
       " 'bake',\n",
       " 'pan',\n",
       " '.',\n",
       " 'for',\n",
       " 'fill',\n",
       " ',',\n",
       " 'slice',\n",
       " 'rhubarb',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'inch',\n",
       " 'thick',\n",
       " 'and',\n",
       " 'toss',\n",
       " 'with',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'cornstarch',\n",
       " 'and',\n",
       " 'ginger',\n",
       " '.',\n",
       " 'set',\n",
       " 'asid',\n",
       " '.',\n",
       " '2',\n",
       " '.',\n",
       " 'To',\n",
       " 'make',\n",
       " 'crumb',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'larg',\n",
       " 'bowl',\n",
       " ',',\n",
       " 'whisk',\n",
       " 'togeth',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'spice',\n",
       " ',',\n",
       " 'salt',\n",
       " 'and',\n",
       " 'butter',\n",
       " 'until',\n",
       " 'smooth',\n",
       " '.',\n",
       " 'stir',\n",
       " 'in',\n",
       " 'flour',\n",
       " 'with',\n",
       " 'a',\n",
       " 'spatula',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'look',\n",
       " 'like',\n",
       " 'a',\n",
       " 'solid',\n",
       " 'dough',\n",
       " '.',\n",
       " '3',\n",
       " '.',\n",
       " 'To',\n",
       " 'prepar',\n",
       " 'cake',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'small',\n",
       " 'bowl',\n",
       " ',',\n",
       " 'stir',\n",
       " 'togeth',\n",
       " 'the',\n",
       " 'sour',\n",
       " 'cream',\n",
       " ',',\n",
       " 'egg',\n",
       " ',',\n",
       " 'egg',\n",
       " 'yolk',\n",
       " 'and',\n",
       " 'vanilla',\n",
       " '.',\n",
       " 'use',\n",
       " 'a',\n",
       " 'mixer',\n",
       " 'fit',\n",
       " 'with',\n",
       " 'paddl',\n",
       " 'attach',\n",
       " ',',\n",
       " 'mix',\n",
       " 'togeth',\n",
       " 'flour',\n",
       " ',',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'bake',\n",
       " 'soda',\n",
       " ',',\n",
       " 'bake',\n",
       " 'powder',\n",
       " 'and',\n",
       " 'salt',\n",
       " '.',\n",
       " 'add',\n",
       " 'butter',\n",
       " 'and',\n",
       " 'a',\n",
       " 'spoon',\n",
       " 'of',\n",
       " 'sour',\n",
       " 'cream',\n",
       " 'mixtur',\n",
       " 'and',\n",
       " 'mix',\n",
       " 'on',\n",
       " 'medium',\n",
       " 'speed',\n",
       " 'until',\n",
       " 'flour',\n",
       " 'is',\n",
       " 'moisten',\n",
       " '.',\n",
       " 'increas',\n",
       " 'speed',\n",
       " 'and',\n",
       " 'beat',\n",
       " 'for',\n",
       " '30',\n",
       " 'second',\n",
       " '.',\n",
       " 'add',\n",
       " 'remain',\n",
       " 'sour',\n",
       " 'cream',\n",
       " 'mixtur',\n",
       " 'in',\n",
       " 'two',\n",
       " 'batch',\n",
       " ',',\n",
       " 'beat',\n",
       " 'for',\n",
       " '20',\n",
       " 'second',\n",
       " 'after',\n",
       " 'each',\n",
       " 'addit',\n",
       " ',',\n",
       " 'and',\n",
       " 'scrape',\n",
       " 'down',\n",
       " 'the',\n",
       " 'side',\n",
       " 'of',\n",
       " 'bowl',\n",
       " 'with',\n",
       " 'a',\n",
       " 'spatula',\n",
       " '.',\n",
       " 'scoop',\n",
       " 'out',\n",
       " 'about',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'cup',\n",
       " 'batter',\n",
       " 'and',\n",
       " 'set',\n",
       " 'asid',\n",
       " '.',\n",
       " '4',\n",
       " '.',\n",
       " 'scrape',\n",
       " 'remain',\n",
       " 'batter',\n",
       " 'into',\n",
       " 'prepar',\n",
       " 'pan',\n",
       " '.',\n",
       " 'spoon',\n",
       " 'rhubarb',\n",
       " 'over',\n",
       " 'batter',\n",
       " '.',\n",
       " 'dollop',\n",
       " 'set',\n",
       " '-',\n",
       " 'asid',\n",
       " 'batter',\n",
       " 'over',\n",
       " 'rhubarb',\n",
       " ';',\n",
       " 'it',\n",
       " 'doe',\n",
       " 'not',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'even',\n",
       " '.',\n",
       " '5',\n",
       " '.',\n",
       " 'use',\n",
       " 'your',\n",
       " 'finger',\n",
       " ',',\n",
       " 'break',\n",
       " 'top',\n",
       " 'mixtur',\n",
       " 'into',\n",
       " 'big',\n",
       " 'crumb',\n",
       " ',',\n",
       " 'about',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'inch',\n",
       " 'to',\n",
       " '3',\n",
       " '/',\n",
       " '4',\n",
       " 'inch',\n",
       " 'in',\n",
       " 'size',\n",
       " '.',\n",
       " 'they',\n",
       " 'do',\n",
       " 'not',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'uniform',\n",
       " ',',\n",
       " 'but',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'most',\n",
       " 'are',\n",
       " 'around',\n",
       " 'that',\n",
       " 'size',\n",
       " '.',\n",
       " 'sprinkl',\n",
       " 'over',\n",
       " 'cake',\n",
       " '.',\n",
       " 'bake',\n",
       " 'cake',\n",
       " 'until',\n",
       " 'a',\n",
       " 'toothpick',\n",
       " 'insert',\n",
       " 'into',\n",
       " 'center',\n",
       " 'come',\n",
       " 'out',\n",
       " 'clean',\n",
       " 'of',\n",
       " 'batter',\n",
       " '(',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'moist',\n",
       " 'from',\n",
       " 'rhubarb',\n",
       " '),',\n",
       " '45',\n",
       " 'to',\n",
       " '55',\n",
       " 'minut',\n",
       " '.',\n",
       " 'cool',\n",
       " 'complet',\n",
       " 'befor',\n",
       " 'serv',\n",
       " '.',\n",
       " 'yield',\n",
       " ':',\n",
       " '6',\n",
       " 'to',\n",
       " '8',\n",
       " 'serv',\n",
       " '.',\n",
       " 'A',\n",
       " 'good',\n",
       " 'appetit',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem\n",
    "# do we need to do stemming?\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "[porter.stem(t) for t in tokens]\n",
    "# [lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/huyue012/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[\"'RHUBARB is an alarmingly sour vegetable passed off as a fruit, but \"\n",
      " 'requiring a huge mound of sugar to effect the transformation.Crumb cake is a '\n",
      " 'huge mound of sugar disguised as a cake, but demanding a bracing '\n",
      " 'counterpoint -- say a swallow of coffee or tea -- to allay its cloying '\n",
      " 'sweetness.These two truths coexisted in my mind without overlapping until I '\n",
      " 'bit into a piece of crumb cake so texturally perfect (soft sliver of cake '\n",
      " 'topped by a deep layer of grape-size crumbs), yet so toothachingly sweet '\n",
      " 'that the only antidote was sucking on the lemon in my seltzer.The sourness '\n",
      " 'of the lemon immediately made me think about the rhubarb I had in the '\n",
      " 'fridge.',\n",
      " 'It occurred to me that, instead of cutting its tartness with a mountain of '\n",
      " \"sugar, why not mix the rhubarb into a crumb cake to cut the cake's \"\n",
      " \"sweetness?It was an ''Aha!''\",\n",
      " 'moment, as when some forebear first paired caviar with Champagne.']\n"
     ]
    }
   ],
   "source": [
    "# sentence segmentation 1, not very good\n",
    "nltk.download('punkt')\n",
    "sents = nltk.sent_tokenize(b)\n",
    "pprint.pprint(sents[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']),\n",
       "       list(['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentences = np.array(list(brown.sents()), dtype=object)\n",
    "fmt = (len(sentences), sum(map(len, sentences)))\n",
    "print(\"Loaded {:,} sentences ({:g} tokens)\".format(*fmt))\n",
    "\n",
    "if shuffle:\n",
    "    rng = np.random.RandomState(shuffle)\n",
    "    rng.shuffle(sentences)  # in-place\n",
    "split_idx = int(split * len(sentences))\n",
    "train_sentences = sentences[:split_idx]\n",
    "test_sentences = sentences[split_idx:]\n",
    "\n",
    "fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
    "print(\"Training set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
    "print(\"Test set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "\n",
    "return train_sentences, test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/huyue012/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19847"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence segmentation 2, use option 2\n",
    "nltk.download('treebank')\n",
    "import sent_segment # py file\n",
    "\n",
    "# need to flattern the list?\n",
    "sentences = sent_segment.segment_sentences(tokens)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Preprocessing\n",
    "leverage w266_common module\n",
    "tokenize --> canonicalize digit --> canonicalize word --> vocabuluary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 10,000 types\n"
     ]
    }
   ],
   "source": [
    "# Helper libraries\n",
    "\n",
    "reload(utils)\n",
    "# words = [w.lower() for w in text] # lower, DG, UNK...\n",
    "# vocab = sorted(set(words))[1:100000]\n",
    "V = 10000\n",
    "vocab = utils.build_vocab(tokens, V)\n",
    "# vocab, train_ids, test_ids = utils.load_data(body, title, split=0.8, V=V, shuffle=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embedding\n",
    "For extractive modeling - sentence ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install tensorflow_hub\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "# embeddings = embed([\n",
    "# \"The quick brown fox jumps over the lazy dog.\",\n",
    "# \"I am a sentence for which I would like to get its embedding\"])\n",
    "# session=tf.Session()\n",
    "# session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "# print (tf.Session().run(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow NMT Tutorial\n",
    "[https://github.com/tensorflow/nmt![image.png](attachment:image.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "With big corpus, we can train embedding from scratch instead of using trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variable_scope' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-07cee90d3159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m embedding_encoder = variable_scope.get_variable(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \"embedding_encoder\", [src_vocab_size, embedding_size], ...)\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Look up embedding:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   encoder_inputs: [max_time, batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'variable_scope' is not defined"
     ]
    }
   ],
   "source": [
    "# Construct embedding layer\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    self.W_in_ = tf.Variable(tf.random_uniform([self.V, self.H], -1.0, 1.0), name=\"W_in\")\n",
    "    # embedding_lookup gives shape (batch_size, max_time, H)\n",
    "    self.x_ = tf.nn.embedding_lookup(self.W_in_, self.input_w_)\n",
    "\n",
    "# Embedding\n",
    "embedding_encoder = variable_scope.get_variable(\n",
    "    \"embedding_encoder\", [src_vocab_size, embedding_size], ...)\n",
    "# Look up embedding:\n",
    "#   encoder_inputs: [max_time, batch_size]\n",
    "#   encoder_emb_inp: [max_time, batch_size, embedding_size]\n",
    "encoder_emb_inp = embedding_ops.embedding_lookup(\n",
    "    embedding_encoder, encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-95c8b7fe5ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build RNN cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run Dynamic RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   encoder_outputs: [max_time, batch_size, num_units]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_units' is not defined"
     ]
    }
   ],
   "source": [
    "# Build RNN cell\n",
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "# Run Dynamic RNN\n",
    "#   encoder_outputs: [max_time, batch_size, num_units]\n",
    "#   encoder_state: [batch_size, num_units]\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_emb_inp,\n",
    "    sequence_length=source_sequence_length, time_major=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder also needs to have access to the source information, to initialize it with the last hidden state of the encoder\n",
    "# Build RNN cell\n",
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, decoder_lengths, time_major=True)\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, encoder_state,\n",
    "    output_layer=projection_layer)\n",
    "# Dynamic decoding\n",
    "outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n",
    "logits = outputs.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = layers_core.Dense(\n",
    "    tgt_vocab_size, use_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_outputs, logits=logits)\n",
    "train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "    batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Below are just placeholders **\n",
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, lm.final_h_, train_op]        \n",
    "        #### YOUR CODE HERE ####\n",
    "        # session.run(...) the ops with the feed_dict constructed above.\n",
    "        # Ensure \"cost\" becomes the value of \"loss\".\n",
    "        # Hint: see \"ops\" for other variables that need updating in this loop.\n",
    "        cost = 0.0\n",
    "        vals = session.run(ops, feed_dict)\n",
    "        cost = vals[0] #loss\n",
    "        h = vals[1] #final_h\n",
    "\n",
    "\n",
    "          \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9d69cea1f2f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtoken_feed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcanonicalize_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtoken_feed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcanonicalize_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kw' is not defined"
     ]
    }
   ],
   "source": [
    "V = 10000\n",
    "\n",
    "from . import vocabulary\n",
    "if isinstance(tokens, list):\n",
    "    token_feed = (canonicalize_word(w) for w in tokens)\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "else:\n",
    "    token_feed = (canonicalize_word(w) for w in tokens.words())\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "\n",
    "print(\"Vocabulary: {:,} types\".format(vocab.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.05 #default 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a3_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.    \n",
    "        cost = run_epoch(lm, session, bi, \n",
    "                 learning_rate=learning_rate, train=True, \n",
    "                 verbose=True, tick_s=10)\n",
    "        print(\"loss: {:.03f}  (perplexity: {:.02f})\".format(cost, np.exp(cost)))\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "#         if epoch == num_epochs:\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print(\"\")\n",
    "\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
